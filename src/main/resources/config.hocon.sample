# Default configuration for kinesis-lzo-s3-sink

# Sources currently supported are:
# 'kinesis' for reading records from a Kinesis stream
# 'NSQ' for reading records from NSQ
source = "{{source}}"

sink {

  # The following are used to authenticate for the Amazon Kinesis sink.
  #
  # If both are set to 'default', the default provider chain is used
  # (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)
  #
  # If both are set to 'iam', use AWS IAM Roles to provision credentials.
  #
  # If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
  aws {
    access-key: "iam"
    secret-key: "iam"
  }

  # Config for NSQ sink and NSQ source
  NSQ {
    #Topic name for good event source
    good-source-topic: "{{sinkKinesisNsqSourceGoodTopicName}}"

    #Channel name for good event source
    good-source-channel: "{{sinkKinesisNsqSourceGoodChannelName}}"

    #Topic name for bad event sink
    bad-sink: "{{sinkKinesisNsqSinkTopicBadName}}"
     
    #Host name for NSQ tools
    nsq-host: "{{NsqHost}}"

    #Port for nsqd
    nsqd-port: "{{NsqdPort}}"

    #Port for nsqlookupd
    nsqlookupd-port: {{NsqlookupdPort}}
  }

  kinesis {
    in {
      # Kinesis input stream name
      stream-name: "{{sinkKinesisInStreamName}}"

      # LATEST: most recent data.
      # TRIM_HORIZON: oldest available data.
      # Note: This only affects the first run of this application
      # on a stream.
      initial-position: "TRIM_HORIZON"

      # Maximum number of records to read per GetRecords call     
      max-records: {{sinkKinesisMaxRecords}}
    }

    out {
      # Stream for events for which the storage process fails
      stream-name: "{{sinkKinesisOutStreamName}}"
    }

    region: "{{sinkKinesisRegion}}"

    # "app-name" is used for a DynamoDB table to maintain stream state.
    # You can set it automatically using: "SnowplowLzoS3Sink-$\\{sink.kinesis.in.stream-name\\}"
    app-name: "{{sinkKinesisAppName}}"
  }

  s3 {
    # If using us-east-1, then endpoint should be "http://s3.amazonaws.com".
    # Otherwise "http://s3-<<region>>.s3.amazonaws.com", e.g.
    # http://s3-eu-west-1.amazonaws.com
    region: "{{sinkKinesisS3Region}}"
    bucket: "{{sinkKinesisS3Bucket}}"

    # Format is one of lzo or gzip
    # Note, that you can use gzip only for enriched data stream.
    format: "{{sinkKinesisFormat}}"

    # Maximum Timeout that the application is allowed to fail for
    max-timeout: {{sinkKinesisMaxTimeout}}
  }

  # Events are accumulated in a buffer before being sent to S3.
  # The buffer is emptied whenever:
  # - the combined size of the stored records exceeds byte-limit or
  # - the number of stored records exceeds record-limit or
  # - the time in milliseconds since it was last emptied exceeds time-limit
  buffer {
    byte-limit: {{sinkLzoBufferByteThreshold}} # Not supported by NSQ; will be ignored
    record-limit: {{sinkLzoBufferRecordThreshold}}
    time-limit: {{sinkLzoBufferTimeThreshold}} # Not supported by NSQ; will be ignored
  }

  # Set the Logging Level for the S3 Sink
  # Options: ERROR, WARN, INFO, DEBUG, TRACE
  logging {
    level: "{{sinkLzoLogLevel}}"
  }

  # Optional section for tracking endpoints
  monitoring {
    snowplow {
      collector-uri: "{{collectorUri}}"
      collector-port: 80
      app-id: "{{sinkLzoAppName}}"
      method: "GET"
    }
  }
}
